{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting data from google drive\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load the CIFAR100 dataset\n",
    "trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Create the dataloaders\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting data from google drive\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "# loading the dataset from google drive\n",
    "\n",
    "#trainset = datasets.MNIST(root='/content/drive/MyDrive/MLP_data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "#testset = datasets.MNIST(root='/content/drive/MyDrive/MLP_data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "# Create the dataloaders\n",
    "#trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "#testloader = DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        # Define the layers of the MLP\n",
    "        self.fc1 = nn.Linear(32*32*3, 512) # input layer\n",
    "        self.fc2 = nn.Linear(512, 256) # hidden layer\n",
    "        self.fc3 = nn.Linear(256, 128) # hidden layer\n",
    "        self.fc4 = nn.Linear(128, num_classes) # output layer\n",
    "        \n",
    "        # Add batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        # Add dropout layers\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Flatten the input images\n",
    "        x = x.view(-1, 32*32*3)\n",
    "        \n",
    "        # Pass the input through the first fully connected layer and apply batch normalization and ReLU activation\n",
    "        x = self.bn1(F.relu(self.fc1(x)))\n",
    "\n",
    "        # Apply dropout to the output of the first fully connected layer\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Pass the output through the second fully connected layer and apply batch normalization and ReLU activation\n",
    "        x = self.bn2(F.relu(self.fc2(x)))\n",
    "        \n",
    "        # Apply dropout to the output of the second fully connected layer\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass the output through the third fully connected layer and apply ReLU activation\n",
    "        x = F.relu(self.fc3(x))\n",
    "        \n",
    "        # Apply dropout to the output of the third fully connected layer\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass the output through the fourth fully connected layer and apply log softmax activation\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization can help to stabilize the training process and reduce overfitting. We added batch normalization layers after the first two fully connected layers.<br>\n",
    "\n",
    "Dropout can help to regularize the model and reduce overfitting. We added dropout layers after the first two fully connected layers and after the third fully connected layer.<br>\n",
    "\n",
    "Instead of using ReLU activation function for all the layers, we added batch normalization layers and used ReLU activation function for the first two fully connected layers, and used only ReLU activation function for the third fully connected layer.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and optimizer\n",
    "# Define the model\n",
    "model = MLP()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/782], Loss: 4.5153\n",
      "Epoch [1/10], Step [200/782], Loss: 4.3956\n",
      "Epoch [1/10], Step [300/782], Loss: 3.8837\n",
      "Epoch [1/10], Step [400/782], Loss: 4.1206\n",
      "Epoch [1/10], Step [500/782], Loss: 4.0125\n",
      "Epoch [1/10], Step [600/782], Loss: 4.0146\n",
      "Epoch [1/10], Step [700/782], Loss: 3.8738\n",
      "Epoch [2/10], Step [100/782], Loss: 4.0349\n",
      "Epoch [2/10], Step [200/782], Loss: 3.8531\n",
      "Epoch [2/10], Step [300/782], Loss: 3.9116\n",
      "Epoch [2/10], Step [400/782], Loss: 3.9661\n",
      "Epoch [2/10], Step [500/782], Loss: 4.0784\n",
      "Epoch [2/10], Step [600/782], Loss: 3.8801\n",
      "Epoch [2/10], Step [700/782], Loss: 3.6288\n",
      "Epoch [3/10], Step [100/782], Loss: 3.9580\n",
      "Epoch [3/10], Step [200/782], Loss: 3.9558\n",
      "Epoch [3/10], Step [300/782], Loss: 3.8788\n",
      "Epoch [3/10], Step [400/782], Loss: 4.0242\n",
      "Epoch [3/10], Step [500/782], Loss: 3.8433\n",
      "Epoch [3/10], Step [600/782], Loss: 3.6007\n",
      "Epoch [3/10], Step [700/782], Loss: 3.8738\n",
      "Epoch [4/10], Step [100/782], Loss: 3.9789\n",
      "Epoch [4/10], Step [200/782], Loss: 3.7609\n",
      "Epoch [4/10], Step [300/782], Loss: 3.9748\n",
      "Epoch [4/10], Step [400/782], Loss: 3.8763\n",
      "Epoch [4/10], Step [500/782], Loss: 4.0305\n",
      "Epoch [4/10], Step [600/782], Loss: 4.1171\n",
      "Epoch [4/10], Step [700/782], Loss: 4.0862\n",
      "Epoch [5/10], Step [100/782], Loss: 3.8942\n",
      "Epoch [5/10], Step [200/782], Loss: 4.0446\n",
      "Epoch [5/10], Step [300/782], Loss: 3.9479\n",
      "Epoch [5/10], Step [400/782], Loss: 3.7219\n",
      "Epoch [5/10], Step [500/782], Loss: 3.6615\n",
      "Epoch [5/10], Step [600/782], Loss: 3.9151\n",
      "Epoch [5/10], Step [700/782], Loss: 3.6865\n",
      "Epoch [6/10], Step [100/782], Loss: 3.6484\n",
      "Epoch [6/10], Step [200/782], Loss: 3.8249\n",
      "Epoch [6/10], Step [300/782], Loss: 3.9109\n",
      "Epoch [6/10], Step [400/782], Loss: 3.8526\n",
      "Epoch [6/10], Step [500/782], Loss: 3.8373\n",
      "Epoch [6/10], Step [600/782], Loss: 4.1370\n",
      "Epoch [6/10], Step [700/782], Loss: 4.1223\n",
      "Epoch [7/10], Step [100/782], Loss: 3.7403\n",
      "Epoch [7/10], Step [200/782], Loss: 3.6059\n",
      "Epoch [7/10], Step [300/782], Loss: 3.8795\n",
      "Epoch [7/10], Step [400/782], Loss: 3.9663\n",
      "Epoch [7/10], Step [500/782], Loss: 3.5932\n",
      "Epoch [7/10], Step [600/782], Loss: 3.8928\n",
      "Epoch [7/10], Step [700/782], Loss: 3.9830\n",
      "Epoch [8/10], Step [100/782], Loss: 3.7288\n",
      "Epoch [8/10], Step [200/782], Loss: 3.8721\n",
      "Epoch [8/10], Step [300/782], Loss: 3.6976\n",
      "Epoch [8/10], Step [400/782], Loss: 3.4182\n",
      "Epoch [8/10], Step [500/782], Loss: 3.8970\n",
      "Epoch [8/10], Step [600/782], Loss: 3.5431\n",
      "Epoch [8/10], Step [700/782], Loss: 3.9828\n",
      "Epoch [9/10], Step [100/782], Loss: 4.1141\n",
      "Epoch [9/10], Step [200/782], Loss: 3.9229\n",
      "Epoch [9/10], Step [300/782], Loss: 3.8249\n",
      "Epoch [9/10], Step [400/782], Loss: 4.0895\n",
      "Epoch [9/10], Step [500/782], Loss: 3.8939\n",
      "Epoch [9/10], Step [600/782], Loss: 3.6327\n",
      "Epoch [9/10], Step [700/782], Loss: 3.8124\n",
      "Epoch [10/10], Step [100/782], Loss: 3.7746\n",
      "Epoch [10/10], Step [200/782], Loss: 3.9338\n",
      "Epoch [10/10], Step [300/782], Loss: 3.9823\n",
      "Epoch [10/10], Step [400/782], Loss: 4.0215\n",
      "Epoch [10/10], Step [500/782], Loss: 3.6394\n",
      "Epoch [10/10], Step [600/782], Loss: 3.7810\n",
      "Epoch [10/10], Step [700/782], Loss: 3.5625\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(trainloader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1_score for the training set: 0.15116660595711442\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Initialize lists\n",
    "y_pred_list = []\n",
    "y_true_list = []\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Iterate over test data\n",
    "for images, labels in trainloader:\n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    # Get predictions from the maximum value\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    # Append predictions\n",
    "    y_pred_list.append(predicted)\n",
    "    # Append ground truths\n",
    "    y_true_list.append(labels)\n",
    "\n",
    "# Convert lists to tensors\n",
    "y_pred_list = torch.cat(y_pred_list).cpu().numpy()\n",
    "y_true_list = torch.cat(y_true_list).cpu().numpy()\n",
    "\n",
    "# Calculate F1 score\n",
    "print(\"The F1_score for the training set:\" ,f1_score(y_true_list, y_pred_list, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 17.7 %\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in testloader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1_score for the testing set: 0.14622218194988856\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists\n",
    "y_pred_list = []\n",
    "y_true_list = []\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Iterate over test data\n",
    "for images, labels in testloader:\n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    # Get predictions from the maximum value\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    # Append predictions\n",
    "    y_pred_list.append(predicted)\n",
    "    # Append ground truths\n",
    "    y_true_list.append(labels)\n",
    "\n",
    "# Convert lists to tensors\n",
    "y_pred_list = torch.cat(y_pred_list).cpu().numpy()\n",
    "y_true_list = torch.cat(y_true_list).cpu().numpy()\n",
    "\n",
    "# Calculate F1 score\n",
    "print(\"The F1_score for the testing set:\" ,f1_score(y_true_list, y_pred_list, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "#torch.save(model.state_dict(), 'model.ckpt')\n",
    "\n",
    "# Load model\n",
    "#model.load_state_dict(torch.load('model.ckpt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b5221f7fd568601ffb692ded55cc6a8f9f720c6422993e13b3964f3c2d5ea0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
